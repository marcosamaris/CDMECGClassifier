# of simply running the entire file at once, copy and paste the logical
# blocks and check the output as we go along.
#
# Joshua Reich (josh@i2pi.com)
# April 2, 2009
#
# First we need to load up some packages to support ML.
# If your system doesn't have the packages, check out
# the install.packages() command.
library(rpart)
library(MASS)
library(class)
library(e1071)
rmulnorm <- function (n, mu, sigma)
{
# A simple function for producing n random samples
# from a multivariate normal distribution with mean mu
# and covariance matrix sigma
M <- t(chol(sigma))
d <- nrow(sigma)
Z <- matrix(rnorm(d*n),d,n)
t(M %*% Z + mu)
}
cm <- function (actual, predicted)
{
# Produce a confusion matrix
t<-table(predicted,actual)
# there is a potential bug in here if columns are tied for ordering
t[apply(t,2,function(c) order(-c)[1]),]
}
# Total number of observations
N <- 1000 * 3
# Number of training observations
Ntrain <- N * 0.7
Ntrain
N
# The data that we will be using for the demonstration consists
# of a mixture of 3 multivariate normal distributions. The goal
# is to come up with a classification system that can tell us,
# given a pair of coordinates, from which distribution the data
# arises.
A <- rmulnorm (N/3, c(1,1), matrix(c(4,-6,-6,18), 2,2))
B <- rmulnorm (N/3, c(8,1), matrix(c(1,0,0,1), 2,2))
C <- rmulnorm (N/3, c(3,8), matrix(c(4,0.5,0.5,2), 2,2))
data <- data.frame(rbind (A,B,C))
colnames(data) <- c('x', 'y')
data$class <- c(rep('A', N/3), rep('B', N/3), rep('C', N/3))
# Lets have a look
plot_it <- function () {
plot (data[,1:2], type='n')
points(A, pch='A', col='red')
points(B, pch='B', col='blue')
points(C, pch='C', col='orange')
}
plot_it()
# Randomly arrange the data and divide it into a training
# and test set.
data <- data[sample(1:N),]
train <- data[1:Ntrain,]
test <- data[(Ntrain+1):N,]
# OK. Lets get to it
# K-Means
# kmeans(), built into the R base package, is an unsupervised
# learning technique. The goal is to cluster the observed data
# into groups. This is achieved by assuming a Euclidean distance
# metric, and finding points which lie at local centroids. All
# points are then assigned to their closest centroid and are
# thus clustered. The algorithmic approach to finding these
# centroids is to pick k points at random then assign all other
# points to the centroids. The algorithm then chooses new
# centroids based on the mean point of the resulting clusters.
# Then with these new centroids, the remaining N-k points are
# reclustered. This repeats until some stopping condition is
# reached.
# This algorithm is quite simple to implement and quite often
# the Euclidean distance metric is inappropriate and I find
# myself re-writing the algorithm using a different measure.
# Here we know, a priori, that there are 3 clusters, so we set
# k = 3.
k <- kmeans(train[,1:2], 3)
plot(train[,1:2], type='n')
text(train[,1:2], as.character(k$cluster))
cm (train$class, k$cluster)
test$predicted_class <- knn(train[,1:2], test[,1:2], train$class, k=1)
(m<-cm(test$class, test$predicted_class))
# Here we demonstrate the bias-variance tradeoff as we increase k.
err <- matrix(nrow=N/100, ncol=2)
for (i in 1:nrow(err))
{
k <- i * 4
test$predicted_class <- knn(train[,1:2], test[,1:2], train$class, k=k)
m<-cm(test$class, test$predicted_class)
err[i,] <- c(k, 1 - sum(diag(m)) / sum(m))
}
plot (err)
kernel <- function (a, b)
{
# Lets make a simple 'Gaussian' like kernel that
# measures the distance between two points, a & b.
exp(-sum((a-b)^2))
}
test$predicted_class <- NA
for (i in 1:nrow(test))
{
print(i)
# The weighted distances from each point in the training set to
# row i in the test set
d<-apply(train[,1:2], 1, function(r) kernel(r, test[i,1:2]))
# The class votes, based on the mean distance
v<-aggregate(d, list(class=train$class), mean)
# Predicted class = the class with the lowest distance
test$predicted_class[i] <- v$class[order(-v$x)[1]]
}
cm(test$class, test$predicted_class)
plot_it()
plot_it()
# Recursive Partitioning / Regression Trees
# rpart() implements an algorithm that attempts to recursively split
# the data such that each split best partitions the space according
# to the classification. In a simple one-dimensional case with binary
# classification, the first split will occur at the point on the line
# where there is the biggest difference between the proportion of
# cases on either side of that point. The algorithm continues to
# split the space until a stopping condition is reached. Once the
# tree of splits is produced it can be pruned using regularization
# parameters that seek to ameliorate overfitting.
(r <- rpart(class ~ x + y, data = train))
plot(r)
text(r)
# Here we look at the confusion matrix and overall error rate from applying
# the tree rules to the training data.
predicted <- as.numeric(apply(predict(r), 1, function(r) order(-r)[1]))
(m <- cm (train$class, predicted))
1 - sum(diag(m)) / sum(m)
# And by comparison, against the test data.
predicted <- as.numeric(apply(predict(r, test[,1:2]), 1, function(r) order(-r)[1]))
(m <- cm (test$class, predicted))
1 - sum(diag(m)) / sum(m)
# PCA - Demonstrating that orthogonal bases are better for trees
# Recursive partitioning splits the space along orthogonal hyperplanes
# that are parallel to the original feature coordinate axes. However,
# in our case, the clusters are not neatly split by such planes and
# better results can be found by transforming to another space. We
# use principle component analysis (PCA) to transform our space.
# PCA transforms the space by looking at the vectors along which the
# bulk of the variance in the data occur. The vector that embodies
# the greatest variance becomes the first principle component axis
# in the transformed space. The second axis then is formed along the
# vector that is orthogonal to the first but with the second most
# variance in the data. And so on.
# It should be clear how this transform improves the performance of
# recursive partitioning, but the cost is that the tree splits
# no longer directly map to the feature space, which makes
# interpretation much more difficult.
p<-princomp(train[,1:2])
train_pca <- data.frame(p$scores)
train_pca$class <- train$class
# Compare the alignment of the clusters to the axis in the feature
# space versus the transformed space.
par(mfrow=c(1,2))
train_lda <- as.matrix(train[,1:2]) %*% l$scaling
plot_it()
plot(train_pca[,1:2], type='n')
text(train_pca[,1:2], train_pca$class)
par(mfrow=c(1,1))
r2 <- rpart(class ~ Comp.1 + Comp.2, data = train_pca)
predicted2 <- as.numeric(apply(predict(r2), 1, function(r) order(-r)[1]))
(m <- cm (train$class, predicted2))
1 - sum(diag(m)) / sum(m)
# LDA
# In linear discriminant analysis we no longer look for recursive
# partitions, but rather for lines that go between the clusters.
# In some ways, this is similar to KNN. LDA makes the assumption
# that the clusters are drawn from multivariate normal distributions
# with different means, but identical covariances. LDA approaches
# the problem by applying a transform that applies the inverse of
# the estimated covariance matrix to distributed the points
# spherically. In this transformed space classification is simply
# a matter of finding the closest cluster mean.
# The assumption of identical covariances doesn't hold for our
# dataset, but still provides an improvement. Quadratic DA drops
# this assumption at the cost of greater complexity.
l <- lda(class ~ x + y, data = train)
(m <- cm(train$class, predict(l)$class))
1 - sum(diag(m)) / sum(m)
par(mfrow=c(1,2))
train_lda <- as.matrix(train[,1:2]) %*% l$scaling
plot_it()
plot(train_lda, type='n')
text(train_lda, train$class)
par(mfrow=c(1,1))
# SVM
# Support vector machines take the next step from LDA/QDA. However
# instead of making linear voronoi boundaries between the cluster
# means, we concern ourselves primarily with the points on the
# boundaries between the clusters. These boundary points define
# the 'support vector'. Between two completely separable clusters
# there are two support vectors and a margin of empty space
# between them. The SVM optimization technique seeks to maximize
# the margin by choosing a hyperplane between the support vectors
# of the opposing clusters. For non-separable clusters, a slack
# constraint is added to allow for a small number of points to
# lie inside the margin space. The Cost parameter defines how
# to choose the optimal classifier given the presence of points
# inside the margin. Using the kernel trick (see Mercer's theorem)
# we can get around the requirement for linear separation
# by representing the mapping from the linear feature space to
# some other non-linear space that maximizes separation. Normally
# a kernel would be used to define this mapping, but with the
# kernel trick, we can represent this kernel as a dot product.
# In the end, we don't even have to define the transform between
# spaces, only the dot product distance metric. This leaves
# this algorithm much the same, but with the addition of
# parameters that define this metric. The default kernel used
# is a radial kernel, similar to the kernel defined in my
# kernel method example. The addition is a term, gamma, to
# add a regularization term to weight the importance of distance.
s <- svm( I(factor(class)) ~ x + y, data = train, cost = 100, gama = 1)
(m <- cm(train$class, predict(s)))
1 - sum(diag(m)) / sum(m)
(m <- cm(test$class, predict(s, test[,1:2])))
1 - sum(diag(m)) / sum(m)
90\2
9/(10)
(9/(10))*100
(1/(10))*100
(1/(2))*100
(1/(10))*100
(4/(4+5))*100
load("~/Dropbox/MS-Tesis/APP/RData/ECGInfartos.RData")
load("~/Dropbox/MS-Tesis/APP/RData/ECGSanos.RData")
library("sos")
remove.packages(sos)
remove.packages("sos")
install.packages("sos")
library("sos")
findFn2xls("wavelet")
findFn("wavelet")
1/((1-.9)+.9/2)
1/((1-.5)+.5/2)
1/((1-.5)+.5/1)
Time <- c(4.894,21.381,83.514,214.945,433.087,860.878,1258.695)
ts.plot(Time)
require(stats)
plot(cars)
lines(lowess(cars))
plot(sin, -pi, 2*pi) # see ?plot.function
## Discrete Distribution Plot:
plot(table(rpois(100,5)), type = "h", col = "red", lwd=10,
main="rpois(100,lambda=5)")
## Simple quantiles/ECDF, see ecdf() {library(stats)} for a better one:
plot(x <- sort(rnorm(47)), type = "s", main = "plot(x, type = \"s\")")
points(x, cex = .5, col = "dark red")
Time <- c(4.894,21.381,83.514,214.945,433.087,860.878,1258.695)
plot(Time,tupe="l")
plot(Time,type="l")
axis(1, at=1:5, lab=c("1","5","20","20","50", "100", "200", "300"))
plot(Time,type="l",)
axis(1, at=1:5, lab=c("1","5","20","20","50", "100", "200", "300"))
axis(1, at=1:5, lab=c("1","5","20","50", "100", "200", "300"))
plot(Time,type="l",)
axis(1, at=1:5, lab=c("1","5","20","50", "100", "200", "300"))
Time <- c(4.894,21.381,83.514,214.945,433.087,860.878,1258.695)
plot(Time,type="l",)
axis(1, at=1:5, lab=c("1","5","20","50", "100", "200", "300"))
plot(Time,type="l",)
axis(1, at=1:7, lab=c("1","5","20","50", "100", "200", "300"))
axis(lab=c("1","5","20","50", "100", "200", "300"))
plot(Time,type="l",)
axis(lab=c("1","5","20","50", "100", "200", "300"))
plot(Time,type="l",)
axis(1, lab=c("1","5","20","50", "100", "200", "300"))
axis(1, at=1:7, lab=c("1","5","20","50", "100", "200", "300"))
axis(1, at=c("1","5","20","50", "100", "200", "300"))
plot(Time,type="l",)
axis(1, at=c("1","5","20","50", "100", "200", "300"))
Time <- c(4.894,21.381,83.514,214.945,433.087,860.878,1258.695)
plot(Time,type="l",)
axis(1, at=c("1","5","20","50", "100", "200", "300"))
axis(1, at=1:7, lab=c("1","5","20","50", "100", "200", "300"))
plot(Time,type="l",axes=FALSE, ann=FALSE)
axis(1, at=1:7, lab=c("1","5","20","50", "100", "200", "300"))
plot(Time,type="l",axes=FALSE)
axis(1, at=1:7, lab=c("1","5","20","50", "100", "200", "300"))
plot(Time,type="l",axes=FALSE)
axis(1, at=1:7, lab=c("1","5","20","50", "100", "200", "300"))
plot(Time,type="l",axes=c("1","5","20","50", "100", "200", "300"), ann=FALSE)
plot(Time,type="l",axes=c("1","5","20","50", "100", "200", "300"), ann=FALSE)
plot(Time,type="l",axes=c("1","5","20","50", "100", "200", "300"))
plot(Time,type="l"ann=FALSE)
plot(Time,type="l",ann=FALSE)
plot(Time,type="l",ann=FALSE, labels=c("1","5","20","50", "100", "200", "300"))
plot(Time,type="l",axes=FALSE, ann=FALSE)
axis(1, at=1:7, lab=c("1","5","20","50", "100", "200", "300"))
axis(2, at=1:7, lab=c("1","5","20","50", "100", "200", "300"))
axis(2, last=1, at=max(Time))
plot(Time,type="l",axes=FALSE, ann=FALSE)
axis(1, at=1:7, lab=c("1","5","20","50", "100", "200", "300"))
axis(2, last=1, at=max(Time))
box()
axis(2, last=1, 0:at=max(Time))
plot(Time,type="l",axes=FALSE, ann=FALSE)
axis(1, at=1:7, lab=c("1","5","20","50", "100", "200", "300"))
axis(2, last=1, 0:at=max(Time))
axis(2, last=1, at=0:max(Time))
box()
plot(Time,type="l",axes=FALSE, ann=FALSE, main="Rapidez")
axis(1, at=1:7, lab=c("1","5","20","50", "100", "200", "300"))
axis(2, last=1, at=0:max(Time))
box()
Time <- c(4.894,21.381,83.514,214.945,433.087,860.878,1258.695)
plot(Time,type="l",axes=FALSE, ann=FALSE)
axis(1, at=1:7, lab=c("1","5","20","50", "100", "200", "300"))
axis(2, last=1, at=0:max(Time))
title(main="Autos", col.main="red", font.main=4)
# Label the x and y axes with dark green text
title(xlab="Days", col.lab=rgb(0,0.5,0))
title(ylab="Total", col.lab=rgb(0,0.5,0))
box()
title(main="Autos", col.main="blue", font.main=4)
title(main="Autos", font.main=4)
title(ylab="Total")
title(ylab="Total")
title(xlab="Days")
plot(Time, type="o", col="blue")
Time <- c(4.894,21.381,83.514,214.945,433.087,860.878,1258.695)
plot(Time,type="l",axes=FALSE, ann=FALSE)
plot(Time, type="o",)
axis(1, at=1:7, lab=c("1","5","20","50", "100", "200", "300"))
axis(2, last=1, at=0:max(Time))
title(main="Autos", font.main=4)
# Label the x and y axes with dark green text
title(xlab="Days")
title(ylab="Total")
box()
plot(Time,type="l",axes=FALSE, ann=FALSE)
plot(Time, type="o",)
plot(Time,type="l",axes=FALSE, ann=FALSE)
plot(Time, type="o")
axis(1, at=1:7, lab=c("1","5","20","50", "100", "200", "300"))
plot(Time,type="l",axes=FALSE, ann=FALSE)
plot(Time, type="o",axes=FALSE, ann=FALSE)
axis(1, at=1:7, lab=c("1","5","20","50", "100", "200", "300"))
axis(2, last=1, at=0:max(Time))
title(main="Autos", font.main=4)
# Label the x and y axes with dark green text
title(xlab="Days")
title(ylab="Total")
box()
Time <- c(4.894,21.381,83.514,214.945,433.087,860.878,1258.695)
plot(Time,type="l",axes=FALSE, ann=FALSE)
plot(Time, type="o",axes=FALSE, ann=FALSE)
axis(1, at=1:7, lab=c("1","5","20","50", "100", "200", "300"))
axis(2, at=0:max(Time))
title(main="Autos", font.main=4)
# Label the x and y axes with dark green text
title(xlab="Days")
title(ylab="Total")
box()
plot(Time,type="l",axes=FALSE, ann=FALSE)
plot(Time, type="o",axes=FALSE, ann=FALSE)
axis(1, at=1:7, lab=c("1","5","20","50", "100", "200", "300"))
axis(2, at=0:max(Time))
title(main="Tiempo total de ejecución de los Algoritmos\n según número de señales", font.main=4)
# Label the x and y axes with dark green text
title(xlab="Days")
title(ylab="Total")
box()
box()
title(xlab="Número de señales")
title(ylab="Tiempo")
plot(Time,type="l",axes=FALSE, ann=FALSE)
plot(Time, type="o",axes=FALSE, ann=FALSE)
axis(1, at=1:7, lab=c("1","5","20","50", "100", "200", "300"))
axis(2, at=0:max(Time))
title(main="Tiempo total de ejecución de los Algoritmos\n según número de señales", font.main=4)
# Label the x and y axes with dark green text
title(xlab="Número de señales")
title(ylab="Tiempo")
box()
library(wmtsa)
wavDaubechies(wavelet="s8", normalized=TRUE)
f <- wavDaubechies(wavelet="d6", normalized=TRUE)
plot(f, type="time")
plot(f, type="gain")
plot(f, type="time")
library("wavelets")
remove.packages("wavelets")
installl.packages("wavelets")
install.packages("wavelets")
library("wavelets")
figure98.wt.filter(filter=
d6)
figure98.wt.filter(filter="d6")
figure98.wt.filter(filter="d6",levels = c(5))
filter <- wt.filter()
figure108.wt.filter(filter)
# Alternatively
figure108.wt.filter("la8")
# Plotting the Haar, D4, D6 Wavelet Filters
figure108.wt.filter(list("haar", "d4", "d6"))
Plotting the Haar, D4, D6 Scaling Filters
figure108.wt.filter(list("haar", "d4", "d6"), wavelet = FALSE)
# Alternatively
haar <- wt.filter("haar")
d6 <- wt.filter("d6")
figure108.wt.filter(list(haar, "d4", d6), wavelet = FALSE)
# Adding an "made up" filter (represented by c(1,-1,1,-1)
figure108.wt.filter(list(haar, "d4", d6, c(1,-1,1,-1)), wavelet = FALSE)
[Package wavelets version
# Plotting LA8 Wavelet Filter Coefficients Levels 1 through 7.
filter <- wt.filter()
figure98.wt.filter(filter)
# Alternatively
figure98.wt.filter("la8")
# Plotting D4 Scaling Filter Coefficients Levels 1, 3, and 5 and not
# vertically normalizing each level to its plotting region.
figure98.wt.filter("d4", levels = c(1,3,5), wavelet = FALSE, y.normalize
= FALSE)
library("waveslim")
## Figure 4.14 in Gencay, Selcuk and Whitcher (2001)
par(mfrow=c(3,1), mar=c(5-2,4,4-1,2))
f.seq <- "HLLLLL"
plot(c(rep(0,33), wavelet.filter("mb4", f.seq), rep(0,33)), type="l",
xlab="", ylab="", main="D(4) in black, MB(4) in red")
lines(c(rep(0,33), wavelet.filter("d4", f.seq), rep(0,33)), col=2)
plot(c(rep(0,35), -wavelet.filter("mb8", f.seq), rep(0,35)), type="l",
xlab="", ylab="", main="D(8) in black, -MB(8) in red")
lines(c(rep(0,35), wavelet.filter("d8", f.seq), rep(0,35)), col=2)
plot(c(rep(0,39), wavelet.filter("mb16", f.seq), rep(0,39)), type="l",
xlab="", ylab="", main="D(16) in black, MB(16) in red")
lines(c(rep(0,39), wavelet.filter("d16", f.seq), rep(0,39)), col=2)
plot(c(rep(0,33), wavelet.filter("d6", f.seq), rep(0,33)))
plot(c(rep(0,33), wavelet.filter("d6", f.seq), rep(0,33)), type="l")
setwd('~/Dropbox/MS-Tesis/APP/')
i=1
setwd("./images/Sanos/")
png(file=paste("San-",i,"-RR",names(ecgSanosF[i]),sep=""), width=1800,height=1000)
plot(seq(from=0,to=300,by=1/250)[1:75000],
ecgSanosF[[i]][1:75000],type="l", , xlab="Tiempo (seg)", ylab="Voltaje (mV)", main=paste("No. ",i," ECG ",names(ecgInfartosF[i]),sep=""))
points((seq.int(from=0,to=300,by=1/250))[1:75000][maxSanos[[i]]$index.max],
(ecgSanosF[[i]])[1:75000][maxSanos[[i]]$index.max],col=2, pch=1)
dev.off()
load("./RData/HRV-todos.RData")
setwd('~/Dropbox/MS-Tesis/APP/')
load("./RData/HRV-todos.RData")
i
png(file=paste("San-",i,"-RR",names(ecgSanosF[i]),sep=""), width=1800,height=1000)
plot(seq(from=0,to=300,by=1/250)[1:75000],
ecgSanosF[[i]][1:75000],type="l", , xlab="Tiempo (seg)", ylab="Voltaje (mV)", main=paste("No. ",i," ECG ",names(ecgInfartosF[i]),sep=""))
points((seq.int(from=0,to=300,by=1/250))[1:75000][maxSanos[[i]]$index.max],
(ecgSanosF[[i]])[1:75000][maxSanos[[i]]$index.max],col=2, pch=1)
dev.off()
for (i in 1:50){
png(file=paste("San-",i,"-hrv",names(ecgSanosF[i]),sep=""), width=1800,height=1000)
plot(hrvSanos[[i]], main=paste("No. ",i," ECG ",names(ecgSanosF[i]),sep=""), type="l")
dev.off()
}
setwd("./images/Sanos/")
png(file=paste("San-",i,"-RR",names(ecgSanosF[i]),sep=""), width=1800,height=1000)
plot(seq(from=0,to=300,by=1/250)[1:75000],
ecgSanosF[[i]][1:75000],type="l", , xlab="Tiempo (seg)", ylab="Voltaje (mV)", main=paste("No. ",i," ECG ",names(ecgInfartosF[i]),sep=""))
points((seq.int(from=0,to=300,by=1/250))[1:75000][maxSanos[[i]]$index.max],
(ecgSanosF[[i]])[1:75000][maxSanos[[i]]$index.max],col=2, pch=1)
dev.off()
for (i in 1:50){
png(file=paste("San-",i,"-RR",names(ecgSanosF[i]),sep=""), width=1800,height=1000)
plot(seq(from=0,to=300,by=1/250)[1:75000],
ecgSanosF[[i]][1:75000],type="l", , xlab="Tiempo (seg)", ylab="Voltaje (mV)", main=paste("No. ",i," ECG ",names(ecgInfartosF[i]),sep=""))
points((seq.int(from=0,to=300,by=1/250))[1:75000][maxSanos[[i]]$index.max],
(ecgSanosF[[i]])[1:75000][maxSanos[[i]]$index.max],col=2, pch=1)
dev.off()
}
setwd("../Infartados/")
for (i in 1:50){
png(file=paste("Inf-",i,"-hrv_",names(ecgInfartosF[i]),sep=""), width=1800,height=1000)
plot(hrvInf[[i]], main=paste("No. ",i," ECG ",names(ecgInfartosF[i]),sep=""), type="l")
dev.off()
}
for (i in 1:50){
png(file=paste("Inf-",i,"-RR_",names(ecgInfartosF[i]),sep=""), width=1800,height=1000)
plot(seq(from=0,to=300,by=1/250)[1:75000],
ecgInfartos[[i]][1:75000],type="l", , xlab="Tiempo (seg)", ylab="Voltaje (mV)", main=paste("No. ",i," ECG ",names(ecgInfartosF[i]),sep=""))
points((seq.int(from=0,to=300,by=1/250))[1:75000][maxInfartos[[i]]$index.max],
(ecgInfartosF[[i]])[1:75000][maxInfartos[[i]]$index.max],col=2, pch=1)
dev.off()
}
setwd("../Sanos/")
for (i in 1:50){
png(file=paste("San-",i,"-hrv_",names(ecgSanosF[i]),sep=""), width=1800,height=1000)
plot(hrvSanos[[i]], main=paste("No. ",i," ECG ",names(ecgSanosF[i]),sep=""), type="l")
dev.off()
}
for (i in 1:50){
png(file=paste("San-",i,"-RR_",names(ecgSanosF[i]),sep=""), width=1800,height=1000)
plot(seq(from=0,to=300,by=1/250)[1:75000],
ecgSanosF[[i]][1:75000],type="l", , xlab="Tiempo (seg)", ylab="Voltaje (mV)", main=paste("No. ",i," ECG ",names(ecgInfartosF[i]),sep=""))
points((seq.int(from=0,to=300,by=1/250))[1:75000][maxSanos[[i]]$index.max],
(ecgSanosF[[i]])[1:75000][maxSanos[[i]]$index.max],col=2, pch=1)
dev.off()
}
setwd("../")
setwd("../Infartados/")
setwd("./Infartados/")
for (i in 1:50){
png(file=paste("Inf-",i,"-RR_",names(ecgInfartosF[i]),sep=""), width=1800,height=1000)
plot(seq(from=0,to=300,by=1/250)[1:75000],
ecgInfartosF[[i]][1:75000],type="l", , xlab="Tiempo (seg)", ylab="Voltaje (mV)", main=paste("No. ",i," ECG ",names(ecgInfartosF[i]),sep=""))
points((seq.int(from=0,to=300,by=1/250))[1:75000][maxInfartos[[i]]$index.max],
(ecgInfartosF[[i]])[1:75000][maxInfartos[[i]]$index.max],col=2, pch=1)
dev.off()
}
